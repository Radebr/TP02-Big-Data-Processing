{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# الخلية 1: الإعداد وتحميل البيانات\n",
        "# ===================================================================\n",
        "\n",
        "# تثبيت المكتبات\n",
        "!pip install -q kaggle dask[complete]\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, time, gzip, shutil, glob, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"📦 TP02: التعامل مع ملفات البيانات الضخمة (5GB+)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ====== إعداد Kaggle API ======\n",
        "print(\"\\n🔐 إعداد Kaggle API...\")\n",
        "print(\"📁 الرجاء رفع ملف kaggle.json:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded_filename = list(uploaded.keys())[0]\n",
        "if uploaded_filename != 'kaggle.json':\n",
        "    os.rename(uploaded_filename, 'kaggle.json')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"✅ تم إعداد Kaggle API\")\n",
        "\n",
        "# ====== تحميل Dataset ======\n",
        "DATASET = \"mkechinov/ecommerce-behavior-data-from-multi-category-store\"\n",
        "print(f\"\\n⏳ جاري تحميل: {DATASET}\")\n",
        "!kaggle datasets download -d {DATASET}\n",
        "\n",
        "# فك الضغط\n",
        "print(\"\\n⏳ جاري فك الضغط...\")\n",
        "zip_file = glob.glob(\"*.zip\")[0]\n",
        "!unzip -q {zip_file}\n",
        "\n",
        "# تحديد الملف\n",
        "DATA_CSV = \"2019-Oct.csv\"\n",
        "file_size_gb = os.path.getsize(DATA_CSV) / (1024**3)\n",
        "print(f\"\\n✅ جاهز! الملف: {DATA_CSV} ({file_size_gb:.2f} GB)\")\n",
        "!head -3 {DATA_CSV}\n",
        "\n",
        "# ====== وظائف مساعدة ======\n",
        "def get_file_size_mb(path):\n",
        "    return os.path.getsize(path) / (1024**2) if os.path.exists(path) else None\n",
        "\n",
        "def measure_time(func, *args, **kwargs):\n",
        "    start = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    return result, time.time() - start\n",
        "\n",
        "print(\"\\n✅ الإعداد مكتمل!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "iJiwArZrMIPc",
        "outputId": "0a1a48f1-121e-4590-9e8a-6685cbcd49c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📦 TP02: التعامل مع ملفات البيانات الضخمة (5GB+)\n",
            "============================================================\n",
            "\n",
            "🔐 إعداد Kaggle API...\n",
            "📁 الرجاء رفع ملف kaggle.json:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ede92a3-1498-41de-b056-3c653178dd37\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ede92a3-1498-41de-b056-3c653178dd37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle(1).json to kaggle(1).json\n",
            "✅ تم إعداد Kaggle API\n",
            "\n",
            "⏳ جاري تحميل: mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
            "Dataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
            "License(s): copyright-authors\n",
            "Downloading ecommerce-behavior-data-from-multi-category-store.zip to /content\n",
            "100% 4.27G/4.29G [00:51<00:00, 252MB/s]\n",
            "100% 4.29G/4.29G [00:52<00:00, 88.5MB/s]\n",
            "\n",
            "⏳ جاري فك الضغط...\n",
            "\n",
            "✅ جاهز! الملف: 2019-Oct.csv (5.28 GB)\n",
            "event_time,event_type,product_id,category_id,category_code,brand,price,user_id,user_session\n",
            "2019-10-01 00:00:00 UTC,view,44600062,2103807459595387724,,shiseido,35.79,541312140,72d76fde-8bb3-4e00-8c23-a032dfed738c\n",
            "2019-10-01 00:00:00 UTC,view,3900821,2053013552326770905,appliances.environment.water_heater,aqua,33.20,554748717,9333dfbd-b87a-4708-9857-6336556b0fcc\n",
            "\n",
            "✅ الإعداد مكتمل!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# الخلية 2: تطبيق الطرق الثلاث والمقارنة\n",
        "# ===================================================================\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"🔹 بدء تنفيذ الطرق الثلاث\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========== الطريقة 1: Pandas Chunks ==========\n",
        "print(\"\\n⏳ الطريقة 1: Pandas Chunks...\")\n",
        "\n",
        "def method_pandas_chunks(file_path, chunksize=100000):\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
        "        total += len(chunk)\n",
        "    return total\n",
        "\n",
        "rows_pandas, time_pandas = measure_time(method_pandas_chunks, DATA_CSV)\n",
        "print(f\"✅ Pandas: {rows_pandas:,} صف في {time_pandas:.2f}s ({rows_pandas/time_pandas:,.0f} صف/ثانية)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Pandas Chunks',\n",
        "    'rows': rows_pandas,\n",
        "    'time_seconds': round(time_pandas, 2),\n",
        "    'speed': round(rows_pandas/time_pandas, 0)\n",
        "})\n",
        "\n",
        "# ========== الطريقة 2: Dask ==========\n",
        "print(\"\\n⏳ الطريقة 2: Dask...\")\n",
        "\n",
        "def method_dask(file_path):\n",
        "    ddf = dd.read_csv(file_path, blocksize=\"64MB\", assume_missing=True)\n",
        "    return ddf.shape[0].compute()\n",
        "\n",
        "rows_dask, time_dask = measure_time(method_dask, DATA_CSV)\n",
        "print(f\"✅ Dask: {rows_dask:,} صف في {time_dask:.2f}s ({rows_dask/time_dask:,.0f} صف/ثانية)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Dask',\n",
        "    'rows': rows_dask,\n",
        "    'time_seconds': round(time_dask, 2),\n",
        "    'speed': round(rows_dask/time_dask, 0)\n",
        "})\n",
        "\n",
        "# ========== الطريقة 3: Compression ==========\n",
        "print(\"\\n⏳ الطريقة 3: Compression...\")\n",
        "\n",
        "COMPRESSED_FILE = DATA_CSV + \".gz\"\n",
        "\n",
        "# ضغط الملف\n",
        "def compress_file(inp, out):\n",
        "    with open(inp, 'rb') as f_in, gzip.open(out, 'wb', compresslevel=6) as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "_, time_compress = measure_time(compress_file, DATA_CSV, COMPRESSED_FILE)\n",
        "\n",
        "original_mb = get_file_size_mb(DATA_CSV)\n",
        "compressed_mb = get_file_size_mb(COMPRESSED_FILE)\n",
        "compression_ratio = ((original_mb - compressed_mb) / original_mb) * 100\n",
        "\n",
        "print(f\"   ضغط: {original_mb:.2f} MB → {compressed_mb:.2f} MB ({compression_ratio:.1f}% توفير) في {time_compress:.2f}s\")\n",
        "\n",
        "# قراءة الملف المضغوط\n",
        "def method_compressed(file_path, chunksize=100000):\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(file_path, compression='gzip', chunksize=chunksize, low_memory=False):\n",
        "        total += len(chunk)\n",
        "    return total\n",
        "\n",
        "rows_comp, time_read = measure_time(method_compressed, COMPRESSED_FILE)\n",
        "total_time = time_compress + time_read\n",
        "\n",
        "print(f\"✅ Compression: {rows_comp:,} صف في {total_time:.2f}s (ضغط: {time_compress:.2f}s + قراءة: {time_read:.2f}s)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Compression',\n",
        "    'rows': rows_comp,\n",
        "    'time_seconds': round(total_time, 2),\n",
        "    'speed': round(rows_comp/total_time, 0),\n",
        "    'original_mb': round(original_mb, 2),\n",
        "    'compressed_mb': round(compressed_mb, 2),\n",
        "    'compression_ratio': round(compression_ratio, 2)\n",
        "})\n",
        "\n",
        "# ========== حفظ النتائج ==========\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('comparison_results.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 مقارنة النتائج\")\n",
        "print(\"=\"*60)\n",
        "print(results_df[['method', 'rows', 'time_seconds', 'speed']].to_string(index=False))\n",
        "\n",
        "print(\"\\n💾 مقارنة المساحة:\")\n",
        "print(f\"   الأصلي: {original_mb:.2f} MB\")\n",
        "print(f\"   المضغوط: {compressed_mb:.2f} MB\")\n",
        "print(f\"   التوفير: {compression_ratio:.1f}%\")\n",
        "\n",
        "fastest = results_df.loc[results_df['time_seconds'].idxmin(), 'method']\n",
        "print(f\"\\n🏆 أسرع طريقة: {fastest}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkboP90MmWU",
        "outputId": "54a128c6-42df-46ef-ba9b-f2b99164304b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "🔹 بدء تنفيذ الطرق الثلاث\n",
            "============================================================\n",
            "\n",
            "⏳ الطريقة 1: Pandas Chunks...\n",
            "✅ Pandas: 42,448,764 صف في 119.42s (355,469 صف/ثانية)\n",
            "\n",
            "⏳ الطريقة 2: Dask...\n",
            "✅ Dask: 42,448,764 صف في 113.44s (374,206 صف/ثانية)\n",
            "\n",
            "⏳ الطريقة 3: Compression...\n",
            "   ضغط: 5406.01 MB → 1661.23 MB (69.3% توفير) في 308.61s\n",
            "✅ Compression: 42,448,764 صف في 462.03s (ضغط: 308.61s + قراءة: 153.42s)\n",
            "\n",
            "============================================================\n",
            "📊 مقارنة النتائج\n",
            "============================================================\n",
            "       method     rows  time_seconds    speed\n",
            "Pandas Chunks 42448764        119.42 355469.0\n",
            "         Dask 42448764        113.44 374206.0\n",
            "  Compression 42448764        462.03  91875.0\n",
            "\n",
            "💾 مقارنة المساحة:\n",
            "   الأصلي: 5406.01 MB\n",
            "   المضغوط: 1661.23 MB\n",
            "   التوفير: 69.3%\n",
            "\n",
            "🏆 أسرع طريقة: Dask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# الخلية 3: إنشاء التقرير وتحميل النتائج\n",
        "# ===================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"📄 إنشاء التقرير النهائي\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ====== إنشاء README.md ======\n",
        "readme_text = f\"\"\"# TP02: التعامل مع ملفات البيانات الضخمة (5GB+)\n",
        "\n",
        "## وصف المشروع\n",
        "تطبيق ثلاث طرق للتعامل مع ملفات CSV الكبيرة في Python:\n",
        "1. Pandas Chunks: قراءة على دفعات\n",
        "2. Dask: حوسبة موزعة\n",
        "3. Compression: ضغط + قراءة\n",
        "\n",
        "## Dataset المستخدم\n",
        "- المصدر: eCommerce Behavior Data\n",
        "- الملف: {DATA_CSV}\n",
        "- الحجم: {file_size_gb:.2f} GB\n",
        "- الصفوف: {rows_pandas:,}\n",
        "\n",
        "## النتائج\n",
        "\n",
        "### مقارنة الأداء\n",
        "| الطريقة | الوقت (ثانية) | السرعة (صف/ثانية) |\n",
        "|---------|---------------|-------------------|\n",
        "| Pandas Chunks | {time_pandas:.2f} | {rows_pandas/time_pandas:,.0f} |\n",
        "| Dask | {time_dask:.2f} | {rows_dask/time_dask:,.0f} |\n",
        "| Compression | {total_time:.2f} | {rows_comp/total_time:,.0f} |\n",
        "\n",
        "### مقارنة المساحة\n",
        "- الأصلي: {original_mb:,.2f} MB\n",
        "- المضغوط: {compressed_mb:,.2f} MB\n",
        "- التوفير: {compression_ratio:.1f}%\n",
        "\n",
        "## الاستنتاجات\n",
        "- أسرع طريقة: {fastest}\n",
        "- الضغط يوفر {compression_ratio:.1f}% من المساحة\n",
        "- كل طريقة مناسبة لسيناريو معين\n",
        "\n",
        "## المتطلبات\n",
        "pandas>=1.3.0\n",
        "dask[complete]>=2021.10.0\n",
        "kaggle>=1.5.0\n",
        "\n",
        "## التاريخ\n",
        "{pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
        "\"\"\"\n",
        "\n",
        "with open('README.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "# ====== إنشاء requirements.txt ======\n",
        "requirements_text = \"\"\"pandas>=1.3.0\n",
        "dask[complete]>=2021.10.0\n",
        "kaggle>=1.5.0\n",
        "numpy>=1.20.0\n",
        "\"\"\"\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements_text)\n",
        "\n",
        "print(\"✅ تم إنشاء README.md و requirements.txt\")\n",
        "\n",
        "# ====== عرض النتائج التفصيلية ======\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📈 النتائج التفصيلية\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# ====== التحليل النهائي ======\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎯 التحليل والتوصيات\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n📊 مقارنة الأداء النسبي:\")\n",
        "print(f\"   • Pandas: {time_pandas:.2f}s (الأساس)\")\n",
        "print(f\"   • Dask: {time_dask:.2f}s ({(time_dask/time_pandas-1)*100:+.1f}%)\")\n",
        "print(f\"   • Compression: {total_time:.2f}s ({(total_time/time_pandas-1)*100:+.1f}%)\")\n",
        "\n",
        "print(f\"\\n💾 توفير المساحة:\")\n",
        "print(f\"   • التوفير: {original_mb - compressed_mb:,.2f} MB ({compression_ratio:.1f}%)\")\n",
        "\n",
        "print(\"\\n✅ التوصيات:\")\n",
        "print(\"   1. للسرعة القصوى: استخدم أسرع طريقة\")\n",
        "print(\"   2. لتوفير المساحة: استخدم الضغط\")\n",
        "print(\"   3. للبيانات الضخمة جداً: استخدم Dask\")\n",
        "print(\"   4. للتطبيقات العامة: Pandas Chunks متوازنة\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml543ucwQ8Il",
        "outputId": "2cd11ed6-9ef8-442f-addd-f26026dca3b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📄 إنشاء التقرير النهائي\n",
            "============================================================\n",
            "✅ تم إنشاء README.md و requirements.txt\n",
            "\n",
            "============================================================\n",
            "📈 النتائج التفصيلية\n",
            "============================================================\n",
            "       method     rows  time_seconds    speed  original_mb  compressed_mb  compression_ratio\n",
            "Pandas Chunks 42448764        119.42 355469.0          NaN            NaN                NaN\n",
            "         Dask 42448764        113.44 374206.0          NaN            NaN                NaN\n",
            "  Compression 42448764        462.03  91875.0      5406.01        1661.23              69.27\n",
            "\n",
            "============================================================\n",
            "🎯 التحليل والتوصيات\n",
            "============================================================\n",
            "\n",
            "📊 مقارنة الأداء النسبي:\n",
            "   • Pandas: 119.42s (الأساس)\n",
            "   • Dask: 113.44s (-5.0%)\n",
            "   • Compression: 462.03s (+286.9%)\n",
            "\n",
            "💾 توفير المساحة:\n",
            "   • التوفير: 3,744.78 MB (69.3%)\n",
            "\n",
            "✅ التوصيات:\n",
            "   1. للسرعة القصوى: استخدم أسرع طريقة\n",
            "   2. لتوفير المساحة: استخدم الضغط\n",
            "   3. للبيانات الضخمة جداً: استخدم Dask\n",
            "   4. للتطبيقات العامة: Pandas Chunks متوازنة\n"
          ]
        }
      ]
    }
  ]
}